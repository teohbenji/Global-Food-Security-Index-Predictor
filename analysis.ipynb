{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_df = pd.read_csv(\"dataset.csv\")\n",
    "# display(uncleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset cleaning\n",
    "\n",
    "#Rename columns to acronyms\n",
    "uncleaned_df = uncleaned_df.rename(columns={\n",
    "    'Global Food Security Index (GFSI)': 'GFSI',\n",
    "    'Human Development Index (HDI)': 'HDI',\n",
    "    'Corruption Index (CI)': 'CI',\n",
    "    'GDP per capita (GDP)': 'GDP',\n",
    "    'Cost of Living Index (COL)': 'COL',\n",
    "    'Healthcare Index (HI)': 'HI'\n",
    "})\n",
    "\n",
    "#Delete rows containing NaN values\n",
    "df = uncleaned_df.replace(\"-\", np.NaN).dropna()\n",
    "df = df.drop(['Country'], axis=1)\n",
    "\n",
    "df = df.astype(float) #convert all values in df to float\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scatter chart showing relationship between HDI and GFSI\n",
    "\n",
    "# plt.scatter(df[\"HDI\"] ,df[\"GFSI\"])\n",
    "# plt.xlabel(\"HDI\")\n",
    "# plt.ylabel(\"GFSI\")\n",
    "# plt.title(\"HDI vs GFSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  Scatter chart showing relationship between CI and GFSI\n",
    "# plt.scatter(df[\"CI\"] ,df[\"GFSI\"])\n",
    "# plt.xlabel(\"CI\")\n",
    "# plt.ylabel(\"GFSI\")\n",
    "# plt.title(\"CI vs GFSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scatter chart showing relationship between GDP and GFSI\n",
    "\n",
    "# plt.scatter(df[\"GDP\"] ,df[\"GFSI\"])\n",
    "# plt.xlabel(\"GDP\")\n",
    "# plt.ylabel(\"GFSI\")\n",
    "# plt.title(\"GDP vs GFSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scatter chart showing relationship between COL and GFSI\n",
    "\n",
    "# plt.scatter(df[\"COL\"] ,df[\"GFSI\"])\n",
    "# plt.xlabel(\"COL\")\n",
    "# plt.ylabel(\"GFSI\")\n",
    "# plt.title(\"COL vs GFSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scatter chart showing relationship between HI and GFSI\n",
    "\n",
    "# plt.scatter(df[\"HI\"] ,df[\"GFSI\"])\n",
    "# plt.xlabel(\"HI\")\n",
    "# plt.ylabel(\"GFSI\")\n",
    "# plt.title(\"HI vs GFSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for preparing test and train datasets\n",
    " \n",
    "def get_features_targets(df, feature_names, target_names):\n",
    "    df_feature = pd.DataFrame(df[feature_names])\n",
    "    df_target = pd.DataFrame(df[target_names])\n",
    "    return df_feature, df_target\n",
    "\n",
    "def normalize_z(dfin, column_means=None, column_stds=None):\n",
    "    if column_means is None:\n",
    "        column_means = dfin.mean(axis=0) # creates 1 mean per column\n",
    "    if column_stds is None:\n",
    "        column_stds = dfin.std(axis=0) # creates 1 std dev per column\n",
    "    dfout = (dfin - column_means)/column_stds\n",
    "        \n",
    "    return dfout, column_means, column_stds\n",
    "\n",
    "def split_data(df_feature, df_target, random_state=None, test_size=0.5):\n",
    "    \n",
    "    indexes = df_feature.index\n",
    "    n = int(len(indexes) * test_size)\n",
    "    \n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    test_indexes = np.random.choice(indexes, n, replace=False)\n",
    "    train_indexes = list(set(indexes) - set(test_indexes))\n",
    "    \n",
    "    df_feature_train = df_feature.loc[train_indexes,:]\n",
    "    df_target_train = df_target.loc[train_indexes,:]\n",
    "    \n",
    "    df_feature_test = df_feature.loc[test_indexes,:]\n",
    "    df_target_test = df_target.loc[test_indexes,:]\n",
    "    \n",
    "    return df_feature_train, df_feature_test, df_target_train, df_target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost and gradient descent functions for regression\n",
    "\n",
    "def calc_linreg(X, beta):\n",
    "    return X @ beta # matrix multiplication\n",
    "\n",
    "def compute_cost_linreg(X, y, beta):\n",
    "    m = X.shape[0] # m is number of values\n",
    "    y_pred = calc_linreg(X,beta)\n",
    "    error = y_pred - y\n",
    "    cost = 1/(2*m) * (error.T @ error) # results in a 2D 1x1 matrix\n",
    "    J = cost[0,0] # extract scalar value from 2D 1x1 matrix\n",
    "    \n",
    "    return J\n",
    "\n",
    "def prepare_feature(df_feature):\n",
    "    m = df_feature.shape[0]\n",
    "\n",
    "    # convert df to numpy array if not df\n",
    "    if isinstance(df_feature, pd.DataFrame):\n",
    "        np_feature = df_feature.to_numpy() \n",
    "    else:\n",
    "        np_feature = df_feature\n",
    "    \n",
    "    # Joins 2D ones array of size (m,1) to the left of np_feature  \n",
    "    X = np.concatenate((np.ones((m,1)), np_feature), axis = 1)\n",
    "    return X\n",
    "\n",
    "def prepare_target(df_target):\n",
    "    # convert df to numpy array if not df\n",
    "    if isinstance(df_target, pd.DataFrame):\n",
    "        return df_target.to_numpy()\n",
    "    else:\n",
    "        return df_target\n",
    "\n",
    "def gradient_descent_linreg(X, y, beta, alpha, num_iters):\n",
    "    m = X.shape[0]\n",
    "    J_storage = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        y_pred = calc_linreg(X,beta)\n",
    "        error = y_pred - y\n",
    "\n",
    "        dev = X.T @ error #partial derivative of J\n",
    "        beta = beta - (alpha/m) * dev # calculate new beta\n",
    "        J_storage.append(compute_cost_linreg(X,y,beta)) #stores cost value at each iteration\n",
    "        \n",
    "    return beta, np.array(J_storage)\n",
    "\n",
    "def predict_linreg(df_feature, beta, means=None, stds=None):\n",
    "    feature,means,stds = normalize_z(df_feature,means,stds)\n",
    "    X = prepare_feature(feature)\n",
    "    y_pred = calc_linreg(X, beta)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to test accuracy of model\n",
    "\n",
    "def r2_score(y, ypred):\n",
    "    error = y - ypred\n",
    "    ssres = np.sum(error**2)\n",
    "\n",
    "    y_mean = np.mean(y)\n",
    "    diff = y - y_mean\n",
    "    sstot = np.sum(diff**2)\n",
    "\n",
    "    return 1 - (ssres/sstot)\n",
    "\n",
    "def mean_squared_error(target, pred):\n",
    "    n = target.shape[0]\n",
    "    error = target - pred\n",
    "    return 1/n * np.sum(error**2)\n",
    "\n",
    "def root_mean_squared_error(target, pred):\n",
    "    n = target.shape[0]\n",
    "    error = target - pred\n",
    "    return pow((1/n * np.sum(error**2)), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression: Predict GFSI using HDI, CI, GDP, COL, HI\n",
    "\n",
    "# Get features and targets from data frame\n",
    "columns = [\"HDI\", \"CI\", \"GDP\", \"COL\", \"HI\"]\n",
    "df_feature, df_target = get_features_targets(df, columns, [\"GFSI\"])\n",
    "\n",
    "# Split the data into training and test data sets\n",
    "df_feature_train, df_feature_test, df_target_train, df_target_test = split_data(df_feature, df_target, random_state = 100, test_size = 0.3)\n",
    "\n",
    "# Normalize the feature using min/max normalization\n",
    "df_feature_train_z,_ ,_ = normalize_z(df_feature_train)\n",
    "\n",
    "X = prepare_feature(df_feature_train_z)\n",
    "target = prepare_target(df_target_train)\n",
    "\n",
    "iterations = 1500\n",
    "alpha = 0.01\n",
    "beta = np.zeros((6,1)) #NOTE: np.zeros((k,1)), k = no. of features + 1 \n",
    "\n",
    "# call the gradient_descent function\n",
    "beta, J_storage = gradient_descent_linreg(X, target, beta, alpha, iterations)\n",
    "\n",
    "# call the predict method to get the predicted values\n",
    "pred = predict_linreg(df_feature_test, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6739953365874148\n",
      "33.950689112636965\n",
      "5.826721986901123\n"
     ]
    }
   ],
   "source": [
    "#Testing of model\n",
    "target = prepare_target(df_target_test) #recalibrate target value to test target values\n",
    "\n",
    "print(r2_score(target, pred))\n",
    "print(mean_squared_error(target, pred))\n",
    "print(root_mean_squared_error(target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
